test compile precise-output
target x86_64

function %f0(i64, i32, i32) -> i32 tail {
  fn0 = colocated %g(i64) breakpoint

block0(v0: i64, v1: i32, v2: i32):
  software_breakpoint fn0, v0+8
  v3 = iadd_imm.i32 v1, 1
  software_breakpoint fn0, v0+9
  v4 = iadd_imm.i32 v2, 1
  software_breakpoint fn0, v0+10
  v5 = iadd v3, v4
  software_breakpoint fn0, v0+11
  return v5
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x10, %rsp
;   movq %r13, (%rsp)
; block0:
;   movq %rdi, %r13
;   software_breakpoint TestCase(%g) %r13+8
;   leal 1(%rsi), %r9d
;   software_breakpoint TestCase(%g) %r13+9
;   software_breakpoint TestCase(%g) %r13+10
;   leal 1(%r9, %rdx), %eax
;   software_breakpoint TestCase(%g) %r13+11
;   movq (%rsp), %r13
;   addq $0x10, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x10, %rsp
;   movq %r13, (%rsp)
; block1: ; offset 0xc
;   movq %rdi, %r13
;   cmpb $0, 8(%r13)
;   je 0x1b
;   callq 0x1b ; reloc_external CallPCRel4 %g -4
;   leal 1(%rsi), %r9d
;   cmpb $0, 9(%r13)
;   je 0x2b
;   callq 0x2b ; reloc_external CallPCRel4 %g -4
;   cmpb $0, 0xa(%r13)
;   je 0x37
;   callq 0x37 ; reloc_external CallPCRel4 %g -4
;   leal 1(%r9, %rdx), %eax
;   cmpb $0, 0xb(%r13)
;   je 0x48
;   callq 0x48 ; reloc_external CallPCRel4 %g -4
;   movq (%rsp), %r13
;   addq $0x10, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %g(i64) breakpoint {
  fn0 = colocated %h(i64) tail
block0(v0: i64):
  call fn0(v0)
  return
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x150, %rsp
;   movq %rax, (%rsp)
;   movq %rcx, 8(%rsp)
;   movq %rdx, 0x10(%rsp)
;   movq %rsi, 0x18(%rsp)
;   movq %rdi, 0x20(%rsp)
;   movq %r8, 0x28(%rsp)
;   movq %r9, 0x30(%rsp)
;   movq %r10, 0x38(%rsp)
;   movq %r11, 0x40(%rsp)
;   movq %r13, 0x48(%rsp)
;   movdqu %xmm0, 0x50(%rsp)
;   movdqu %xmm1, 0x60(%rsp)
;   movdqu %xmm2, 0x70(%rsp)
;   movdqu %xmm3, 0x80(%rsp)
;   movdqu %xmm4, 0x90(%rsp)
;   movdqu %xmm5, 0xa0(%rsp)
;   movdqu %xmm6, 0xb0(%rsp)
;   movdqu %xmm7, 0xc0(%rsp)
;   movdqu %xmm8, 0xd0(%rsp)
;   movdqu %xmm9, 0xe0(%rsp)
;   movdqu %xmm10, 0xf0(%rsp)
;   movdqu %xmm11, 0x100(%rsp)
;   movdqu %xmm12, 0x110(%rsp)
;   movdqu %xmm13, 0x120(%rsp)
;   movdqu %xmm14, 0x130(%rsp)
;   movdqu %xmm15, 0x140(%rsp)
; block0:
;   movq %r13, %rdi
;   call    TestCase(%h)
;   movq (%rsp), %rax
;   movq 8(%rsp), %rcx
;   movq 0x10(%rsp), %rdx
;   movq 0x18(%rsp), %rsi
;   movq 0x20(%rsp), %rdi
;   movq 0x28(%rsp), %r8
;   movq 0x30(%rsp), %r9
;   movq 0x38(%rsp), %r10
;   movq 0x40(%rsp), %r11
;   movq 0x48(%rsp), %r13
;   movdqu 0x50(%rsp), %xmm0
;   movdqu 0x60(%rsp), %xmm1
;   movdqu 0x70(%rsp), %xmm2
;   movdqu 0x80(%rsp), %xmm3
;   movdqu 0x90(%rsp), %xmm4
;   movdqu 0xa0(%rsp), %xmm5
;   movdqu 0xb0(%rsp), %xmm6
;   movdqu 0xc0(%rsp), %xmm7
;   movdqu 0xd0(%rsp), %xmm8
;   movdqu 0xe0(%rsp), %xmm9
;   movdqu 0xf0(%rsp), %xmm10
;   movdqu 0x100(%rsp), %xmm11
;   movdqu 0x110(%rsp), %xmm12
;   movdqu 0x120(%rsp), %xmm13
;   movdqu 0x130(%rsp), %xmm14
;   movdqu 0x140(%rsp), %xmm15
;   addq $0x150, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x150, %rsp
;   movq %rax, (%rsp)
;   movq %rcx, 8(%rsp)
;   movq %rdx, 0x10(%rsp)
;   movq %rsi, 0x18(%rsp)
;   movq %rdi, 0x20(%rsp)
;   movq %r8, 0x28(%rsp)
;   movq %r9, 0x30(%rsp)
;   movq %r10, 0x38(%rsp)
;   movq %r11, 0x40(%rsp)
;   movq %r13, 0x48(%rsp)
;   movdqu %xmm0, 0x50(%rsp)
;   movdqu %xmm1, 0x60(%rsp)
;   movdqu %xmm2, 0x70(%rsp)
;   movdqu %xmm3, 0x80(%rsp)
;   movdqu %xmm4, 0x90(%rsp)
;   movdqu %xmm5, 0xa0(%rsp)
;   movdqu %xmm6, 0xb0(%rsp)
;   movdqu %xmm7, 0xc0(%rsp)
;   movdqu %xmm8, 0xd0(%rsp)
;   movdqu %xmm9, 0xe0(%rsp)
;   movdqu %xmm10, 0xf0(%rsp)
;   movdqu %xmm11, 0x100(%rsp)
;   movdqu %xmm12, 0x110(%rsp)
;   movdqu %xmm13, 0x120(%rsp)
;   movdqu %xmm14, 0x130(%rsp)
;   movdqu %xmm15, 0x140(%rsp)
; block1: ; offset 0xcb
;   movq %r13, %rdi
;   callq 0xd3 ; reloc_external CallPCRel4 %h -4
;   movq (%rsp), %rax
;   movq 8(%rsp), %rcx
;   movq 0x10(%rsp), %rdx
;   movq 0x18(%rsp), %rsi
;   movq 0x20(%rsp), %rdi
;   movq 0x28(%rsp), %r8
;   movq 0x30(%rsp), %r9
;   movq 0x38(%rsp), %r10
;   movq 0x40(%rsp), %r11
;   movq 0x48(%rsp), %r13
;   movdqu 0x50(%rsp), %xmm0
;   movdqu 0x60(%rsp), %xmm1
;   movdqu 0x70(%rsp), %xmm2
;   movdqu 0x80(%rsp), %xmm3
;   movdqu 0x90(%rsp), %xmm4
;   movdqu 0xa0(%rsp), %xmm5
;   movdqu 0xb0(%rsp), %xmm6
;   movdqu 0xc0(%rsp), %xmm7
;   movdqu 0xd0(%rsp), %xmm8
;   movdqu 0xe0(%rsp), %xmm9
;   movdqu 0xf0(%rsp), %xmm10
;   movdqu 0x100(%rsp), %xmm11
;   movdqu 0x110(%rsp), %xmm12
;   movdqu 0x120(%rsp), %xmm13
;   movdqu 0x130(%rsp), %xmm14
;   movdqu 0x140(%rsp), %xmm15
;   addq $0x150, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

